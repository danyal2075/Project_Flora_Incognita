{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4782a0-c589-448a-8d0c-6e6542467169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.datasets import ImageFolder\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e110448-5ec3-4958-9f7a-32f849ef2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'NewPlantDiease/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)'\n",
    "train_data1 = path + '/train'\n",
    "val_data1 = path + '/valid'\n",
    "diease = os.listdir(train_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "437bd21b-7d80-4a16-98bd-24a049585b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [1, 1, 1]\n",
    "transforms = transform.Compose([transform.Resize((256, 256)), transform.ToTensor()])\n",
    "train_datasets = ImageFolder(root = train_data1, transform=transforms)\n",
    "val_datasets = ImageFolder(root = val_data1, transform=transforms)\n",
    "Train_dataloader = DataLoader(dataset = train_datasets,  batch_size= 512, shuffle = False)\n",
    "Val_dataloader = DataLoader(dataset = val_datasets,  batch_size= 256, shuffle = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46a171b-3ab2-4548-ba44-76f727125e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70295 17572\n"
     ]
    }
   ],
   "source": [
    "LEN_TRAIN = len(train_datasets)\n",
    "LEN_val = len(val_datasets)\n",
    "print(LEN_TRAIN, LEN_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e0157c7-06bd-4b9a-b8cd-3c4bbc021b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6549, 0.6510, 0.6471,  ..., 0.7255, 0.7176, 0.7098],\n",
       "         [0.6667, 0.6627, 0.6588,  ..., 0.7294, 0.7216, 0.7176],\n",
       "         [0.6667, 0.6627, 0.6588,  ..., 0.7373, 0.7294, 0.7255],\n",
       "         ...,\n",
       "         [0.3922, 0.4000, 0.4078,  ..., 0.4941, 0.4941, 0.4941],\n",
       "         [0.3961, 0.4000, 0.4039,  ..., 0.4784, 0.4824, 0.4824],\n",
       "         [0.4078, 0.4039, 0.4000,  ..., 0.4745, 0.4784, 0.4824]],\n",
       "\n",
       "        [[0.6392, 0.6353, 0.6314,  ..., 0.7137, 0.7059, 0.6980],\n",
       "         [0.6510, 0.6471, 0.6431,  ..., 0.7176, 0.7098, 0.7059],\n",
       "         [0.6510, 0.6471, 0.6431,  ..., 0.7255, 0.7176, 0.7137],\n",
       "         ...,\n",
       "         [0.3529, 0.3608, 0.3686,  ..., 0.4314, 0.4314, 0.4314],\n",
       "         [0.3569, 0.3608, 0.3647,  ..., 0.4157, 0.4196, 0.4196],\n",
       "         [0.3686, 0.3647, 0.3608,  ..., 0.4118, 0.4157, 0.4196]],\n",
       "\n",
       "        [[0.7686, 0.7647, 0.7608,  ..., 0.8275, 0.8196, 0.8118],\n",
       "         [0.7804, 0.7765, 0.7725,  ..., 0.8314, 0.8235, 0.8196],\n",
       "         [0.7804, 0.7765, 0.7725,  ..., 0.8392, 0.8314, 0.8275],\n",
       "         ...,\n",
       "         [0.4863, 0.4941, 0.5020,  ..., 0.5373, 0.5373, 0.5373],\n",
       "         [0.4902, 0.4941, 0.4980,  ..., 0.5216, 0.5255, 0.5255],\n",
       "         [0.5020, 0.4980, 0.4941,  ..., 0.5176, 0.5216, 0.5255]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image , label = next(iter(Train_dataloader))\n",
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df9b3fb8-2a5d-4122-9153-fdfba9cbbdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [02:26<00:00,  1.06s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.4757, 0.5001, 0.4264]), tensor([0.2165, 0.1957, 0.2320]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating mean and STD\n",
    "import numpy as np\n",
    "s1 = torch.tensor([0.0, 0.0, 0.0])\n",
    "s2 = torch.tensor([0.0, 0.0, 0.0])\n",
    "for image,_ in tqdm(Train_dataloader):\n",
    "    # print(image.size())\n",
    "    s1 += image.sum(axis=[0,2,3])\n",
    "    s2 += (image**2).sum(axis=[0,2,3])\n",
    "n = LEN_TRAIN * image.size(2) * image.size(3)\n",
    "\n",
    "mean = s1 / n\n",
    "std = np.sqrt((s2/n) - ((s1/n)**2))\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a083147d-d004-48ac-99ce-df273bc5ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70295 17572\n"
     ]
    }
   ],
   "source": [
    "LEN_TRAIN = len(train_datasets)\n",
    "LEN_val = len(val_datasets)\n",
    "print(LEN_TRAIN, LEN_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175e9f24-f637-4159-bc00-0b93c311301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(dataloader))\n",
    "# print(images.size())\n",
    "# # Access the first image and label\n",
    "# first_image = images[2]\n",
    "# first_label = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74dbe45-5714-436d-bd31-d7277b310ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(first_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c279bf-ac78-40be-b8c3-1dd3165eb8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "model = models.resnet18(pretrained=True)\n",
    "# model\n",
    "# Freeze all parameters in the pre-trained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# # Extracting features from last CNN layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features , 38)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c1c2b32-56a1-412d-98ce-047a9276140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss function\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a025a-3395-4240-96e0-0c4f546efdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 12/275 [00:56<20:24,  4.65s/batch]"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    start = time.time()\n",
    "    \n",
    "    tr_acc = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(Train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for xtrain, ytrain in tepoch:\n",
    "            \n",
    "            train_prob = model(xtrain)\n",
    "        \n",
    "            \n",
    "            train_loss = loss_function(train_prob, ytrain)\n",
    "            train_loss.backward() # backpropagation\n",
    "\n",
    "            # loss.backward()\n",
    "            optimizer.step() # update the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            # training ends\n",
    "            \n",
    "            train_pred = torch.max(train_prob, 1).indices\n",
    "            tr_acc += int(torch.sum(train_pred == ytrain))\n",
    "            \n",
    "        ep_tr_acc = tr_acc / LEN_TRAIN\n",
    "    end = time.time()\n",
    "    duration = (end - start) / 60\n",
    "    \n",
    "    # print(f\"Epoch: {epoch}, Test_Loss: {train_loss}, Train_acc: {ep_tr_acc}\")\n",
    "    # Evaluate\n",
    "    \n",
    "#     model.eval()\n",
    "#     # with torch.no_grad():\n",
    "    with torch.no_grad():\n",
    "        for xval, yval in Val_dataloader:\n",
    "            val_prob = model(xval)\n",
    "\n",
    "            \n",
    "            val_pred = torch.max(val_prob,1).indices\n",
    "            val_acc += int(torch.sum(val_pred == yval))\n",
    "            \n",
    "        ep_val_acc = val_acc / LEN_val\n",
    "    \n",
    "    end = time.time()\n",
    "    duration = (end - start) / 60\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Test_Loss: {train_loss}, Train_acc: {ep_tr_acc}, Val_acc: {ep_val_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01acfd2-6b1e-4b82-a52b-5f60e20fc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in Train_dataloader:\n",
    "    y_prob = model(x)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a541a300-c21e-4ee3-a6e1-74cf4b9c68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48083dff-ae37-473a-b7fb-2b404c50511b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 21, 31, 15, 30, 21, 31, 21, 22, 21, 21, 22, 15, 30, 31, 31])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.max(y_prob,1).indices\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e60e4165-d04d-4ee5-9884-d08c430fe321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 23, 17, 13, 16, 37, 31, 36,  1, 37, 24, 35, 21,  3,  2,  4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0d98cea-f5ba-4037-9d73-87c2240c0c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 38])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87a417d6-6d70-49ac-84b2-42f4a34e08c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(torch.sum(pred == y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7116ca1-437d-4fa1-bfc9-4c6895b63b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Number of epochs,\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "# # iterate over data\n",
    "# # measure accuarcy \n",
    "# # measure loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da350c3-f445-4b9e-a6be-0f6a4ab4457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel = 0\n",
    "# s1 = 0\n",
    "# s2 = 0\n",
    "# for image, _ in dataloader:\n",
    "#     n = images.size(2) * images.size(3)\n",
    "#     # print(n)\n",
    "#     s1 += n\n",
    "#     s2 += s1**2\n",
    "# print(np.sqrt((s2/n) - (s1/n)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d7c7541-ea01-4ba7-8d32-2570bd49346a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0.]), array([0., 0., 0.]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = np.zeros(3)\n",
    "s2 = np.zeros(3)\n",
    "s1,s2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8dd24-97df-4e4b-9ba8-a500c584c537",
   "metadata": {},
   "source": [
    "# EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce27ed83-13d7-490a-a03a-4ec874a2e311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 3515, 3515)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = [0.4757, 0.5001, 0.4264]\n",
    "std = [0.2165, 0.1957, 0.2320]\n",
    "transforms = transform.Compose([transform.Resize((256, 256)), transform.ToTensor(), transform.Normalize(mean=mean, std=std)])\n",
    "train_datasets = ImageFolder(root = train_data1, transform=transforms)\n",
    "Len_TRAIN = len(train_datasets)\n",
    "train_data = int(0.95 * Len_TRAIN)\n",
    "testing_data = Len_TRAIN - train_data\n",
    "train_set, val_set = torch.utils.data.random_split(train_datasets, [train_data, testing_data])\n",
    "Train_dataloader = DataLoader(dataset = val_set,  batch_size= 256, shuffle = True )\n",
    "LEN_TRAIN = len(val_set)\n",
    "len(Train_dataloader), len(val_set),  LEN_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde49c38-4440-448a-94bd-1c32f11f6872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 1758, 1758)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_datasets = ImageFolder(root = val_data1, transform=transforms)\n",
    "Len_val = len(val_datasets)\n",
    "train_data1 = int(0.9 * Len_val)\n",
    "testing_data1 = len(val_datasets) - train_data1\n",
    "train_set1, val_set1 = torch.utils.data.random_split(val_datasets, [train_data1, testing_data1])\n",
    "Val_dataloader = DataLoader(dataset = val_set1,  batch_size= 256, shuffle = False )\n",
    "LEN_val = len(val_set1)\n",
    "validation_set1 = val_set1.dataset\n",
    "len(validation_set1.classes),len(val_set1), LEN_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb5ac73-9879-4681-8cfe-fa91c5f49a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "model = models.resnet18(pretrained=True)\n",
    "# model\n",
    "# Freeze all parameters in the pre-trained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# # Extracting features from last CNN layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features , 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a82f38-0dd3-4b79-b757-a4c808fd0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.pateince  = patience\n",
    "        self.best_score = None\n",
    "        self.counter = None\n",
    "        self.stop  = False\n",
    "    def _stopping(self,val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.counter == self.pateince:\n",
    "                self.stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            print(self.best_score)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d0ecd5-ac1a-40a2-9f67-168ea6ccf9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss function\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-5)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa011c-ce92-4b0d-b165-5c8af0d03f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:42<00:00, 11.58s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Test_Loss: 3.6161789894104004,Validation_loss: 3.6524455547332764, Train_acc: 0.030440967283072547, Val_acc: 0.03185437997724687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:39<00:00, 11.39s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3905)\n",
      "Epoch: 1, Test_Loss: 3.333961248397827,Validation_loss: 3.390486001968384, Train_acc: 0.06770981507823613, Val_acc: 0.0813424345847554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:40<00:00, 11.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1386)\n",
      "Epoch: 2, Test_Loss: 2.9754586219787598,Validation_loss: 3.1386358737945557, Train_acc: 0.16813655761024182, Val_acc: 0.19681456200227532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:41<00:00, 11.53s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8904)\n",
      "Epoch: 3, Test_Loss: 2.750515937805176,Validation_loss: 2.8904316425323486, Train_acc: 0.3322901849217639, Val_acc: 0.3304891922639363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:38<00:00, 11.30s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6548)\n",
      "Epoch: 4, Test_Loss: 2.444540023803711,Validation_loss: 2.654801607131958, Train_acc: 0.49246088193456616, Val_acc: 0.48862343572241185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:39<00:00, 11.38s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4242)\n",
      "Epoch: 5, Test_Loss: 2.216295003890991,Validation_loss: 2.4241750240325928, Train_acc: 0.6219061166429587, Val_acc: 0.5944254835039818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:38<00:00, 11.35s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2066)\n",
      "Epoch: 6, Test_Loss: 1.950465202331543,Validation_loss: 2.206636905670166, Train_acc: 0.705547652916074, Val_acc: 0.6564277588168373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:37<00:00, 11.28s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0066)\n",
      "Epoch: 7, Test_Loss: 1.7792837619781494,Validation_loss: 2.0065698623657227, Train_acc: 0.764153627311522, Val_acc: 0.7047781569965871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:38<00:00, 11.34s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8205)\n",
      "Epoch: 8, Test_Loss: 1.7180092334747314,Validation_loss: 1.8204705715179443, Train_acc: 0.8091038406827881, Val_acc: 0.7519908987485779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:38<00:00, 11.34s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6598)\n",
      "Epoch: 9, Test_Loss: 1.4611036777496338,Validation_loss: 1.6598339080810547, Train_acc: 0.8401137980085348, Val_acc: 0.7764505119453925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:38<00:00, 11.30s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5219)\n",
      "Epoch: 10, Test_Loss: 1.2847974300384521,Validation_loss: 1.5218993425369263, Train_acc: 0.8654338549075391, Val_acc: 0.8020477815699659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:38<00:00, 11.29s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3925)\n",
      "Epoch: 11, Test_Loss: 1.1881040334701538,Validation_loss: 1.3924964666366577, Train_acc: 0.8867709815078236, Val_acc: 0.8293515358361775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:37<00:00, 11.25s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2816)\n",
      "Epoch: 12, Test_Loss: 1.0882892608642578,Validation_loss: 1.2815687656402588, Train_acc: 0.9046941678520626, Val_acc: 0.8481228668941979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:38<00:00, 11.36s/batch]\n"
     ]
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(patience=3)\n",
    "for epoch in range(50):\n",
    "    start = time.time()\n",
    "    \n",
    "    tr_acc = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(Train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for xtrain, ytrain in tepoch:\n",
    "            \n",
    "            train_prob = model(xtrain)\n",
    "        \n",
    "            \n",
    "            train_loss = loss_function(train_prob, ytrain)\n",
    "            train_loss.backward() # backpropagation\n",
    "\n",
    "            # loss.backward()\n",
    "            optimizer.step() # update the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            # training ends\n",
    "            \n",
    "            train_pred = torch.max(train_prob, 1).indices\n",
    "            tr_acc += int(torch.sum(train_pred == ytrain))\n",
    "            \n",
    "        ep_tr_acc = tr_acc / LEN_TRAIN\n",
    "    end = time.time()\n",
    "    duration = (end - start) / 60\n",
    "    \n",
    "    # print(f\"Epoch: {epoch}, Test_Loss: {train_loss}, Train_acc: {ep_tr_acc}\")\n",
    "    # Evaluate\n",
    "    \n",
    "    model.eval()\n",
    "#     # with torch.no_grad():\n",
    "    with torch.no_grad():\n",
    "        for xval, yval in Val_dataloader:\n",
    "            val_prob = model(xval)\n",
    "            \n",
    "            val_loss = loss_function(val_prob , yval)\n",
    "            \n",
    "            val_pred = torch.max(val_prob,1).indices\n",
    "            val_acc += int(torch.sum(val_pred == yval))\n",
    "            \n",
    "        ep_val_acc = val_acc / LEN_val\n",
    "    avg_val_loss = val_loss / len(Val_dataloader)\n",
    "    earlystopping._stopping(val_loss)\n",
    "    if earlystopping.stop:\n",
    "        print('Early Stopping Excuated')\n",
    "        break\n",
    "    end = time.time()\n",
    "    duration = (end - start) / 60\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Test_Loss: {train_loss},Validation_loss: {val_loss}, Train_acc: {ep_tr_acc}, Val_acc: {ep_val_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936315c1-493c-4a59-a632-1b6dced33ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c54c2-34b2-4796-803f-97c0f103969d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5990eb-6f0a-4d70-aa1d-05957204d9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
