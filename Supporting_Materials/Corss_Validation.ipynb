{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1951d65-246c-4779-9dc7-ade09da6ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.datasets import ImageFolder\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0641a41b-463a-48cd-ad1f-270fbca5d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# NewPlantDiease/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/train\n",
    "path = '/home/NewPlantDiease/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)'\n",
    "train_data1 = path + '/train'\n",
    "val_data1 = path + '/valid'\n",
    "diease = os.listdir(train_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fda9d6e-ce16-45a8-85fb-fff53df7027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************* 0\n",
      "********************* 1\n",
      "********************* 2\n",
      "********************* 3\n",
      "********************* 4\n",
      "********************* 5\n",
      "********************* 6\n",
      "********************* 7\n",
      "********************* 8\n",
      "********************* 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12654, 25, 1405, 12654)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = [0.4757, 0.5001, 0.4264]\n",
    "std = [0.2165, 0.1957, 0.2320]\n",
    "\n",
    "transforms = transform.Compose([transform.Resize((256,256)), transform.ToTensor(), transform.Normalize(mean = mean, std = std)])\n",
    "val_datasets = ImageFolder(root = train_data1, transform=transforms)\n",
    "train_data1 = int(0.2 * len(val_datasets))\n",
    "testing_data1 = len(val_datasets) - train_data1\n",
    "train_set1, val_set1 = torch.utils.data.random_split(val_datasets, [train_data1, testing_data1])\n",
    "kfold = KFold(n_splits=10)\n",
    "avg_train_acc = []\n",
    "avg_test_acc = []\n",
    "avg_train_loss = []\n",
    "avg_test_loss = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_set1)):\n",
    "    print('*********************',fold)\n",
    "    train_sampler = SubsetRandomSampler(train_ids)\n",
    "    \n",
    "    test_sampler = SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    Train_dataloader = DataLoader(train_set1, batch_size = 512, sampler = train_sampler)\n",
    "    Val_dataloader = DataLoader(train_set1, batch_size = 512, sampler = test_sampler)\n",
    "len(train_ids),len(Train_dataloader),len(test_sampler), len(train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d107981-a0a1-445a-a467-d6731d3f3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.pateince  = patience\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.stop  = False\n",
    "    def _stopping(self,val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.counter == self.pateince:\n",
    "                self.stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            # print(self.best_score)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d588a3f-ad8b-44f6-815e-848e97068aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet():\n",
    "    from torchvision import models\n",
    "    import torch.nn as nn\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    # model\n",
    "    # Freeze all parameters in the pre-trained model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    # # Extracting features from last CNN layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features , 38)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a8b87d4-0722-40a4-b0e4-747887d9e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch,Train_dataloader,val_dataloader):\n",
    "    from torchvision import models\n",
    "    import torch.nn as nn\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    # model\n",
    "    # Freeze all parameters in the pre-trained model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    # # Extracting features from last CNN layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features , 38)\n",
    "    # import torch.optim as optim\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-5)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    earlystopping = EarlyStopping(patience=5)\n",
    "    epoch = epoch\n",
    "    Train_dataloader = Train_dataloader\n",
    "    Val_dataloader = val_dataloader\n",
    "    for epoch in range(epoch):\n",
    "        start = time.time()\n",
    "    \n",
    "        tr_acc = 0\n",
    "        val_acc = 0\n",
    "#         avg_train_acc = []\n",
    "#         avg_test_acc = []\n",
    "#         avg_train_loss = []\n",
    "#         avg_test_loss = []\n",
    "    \n",
    "    # Train\n",
    "        model.train()\n",
    "    \n",
    "        with tqdm(Train_dataloader, unit=\"batch\") as tepoch:\n",
    "            for xtrain, ytrain in tepoch:\n",
    "            \n",
    "                train_prob = model(xtrain)\n",
    "        \n",
    "            \n",
    "                train_loss = loss_function(train_prob, ytrain)\n",
    "                train_loss.backward() # backpropagation\n",
    "\n",
    "            # loss.backward()\n",
    "                optimizer.step() # update the model parameters\n",
    "                optimizer.zero_grad()\n",
    "            # training ends\n",
    "            \n",
    "                train_pred = torch.max(train_prob, 1).indices\n",
    "                tr_acc += int(torch.sum(train_pred == ytrain))\n",
    "            \n",
    "            ep_tr_acc = tr_acc / len(Train_dataloader)\n",
    "        end = time.time()\n",
    "        duration = (end - start) / 60\n",
    "        \n",
    "    # print(f\"Epoch: {epoch}, Test_Loss: {train_loss}, Train_acc: {ep_tr_acc}\")\n",
    "    # Evaluate\n",
    "    \n",
    "        model.eval()\n",
    "#     # with torch.no_grad():\n",
    "        with torch.no_grad():\n",
    "            for xval, yval in Val_dataloader:\n",
    "                val_prob = model(xval)\n",
    "            \n",
    "                val_loss = loss_function(val_prob , yval)\n",
    "            \n",
    "                val_pred = torch.max(val_prob,1).indices\n",
    "                val_acc += int(torch.sum(val_pred == yval))\n",
    "            \n",
    "            ep_val_acc = val_acc / len(Val_dataloader)\n",
    "        avg_val_loss = val_loss / len(Val_dataloader)\n",
    "        earlystopping._stopping(val_loss)\n",
    "        if earlystopping.stop:\n",
    "            print('Early Stopping Excuated')\n",
    "            break\n",
    "        end = time.time()\n",
    "        duration = (end - start) / 60\n",
    "    \n",
    "        print(f\"Epoch: {epoch}, Test_Loss: {train_loss},Validation_loss: {val_loss}, Train_acc: {ep_tr_acc}, Val_acc: {ep_val_acc}\")\n",
    "        \n",
    "    avg_train_acc.append(ep_tr_acc)\n",
    "    avg_test_acc.append(ep_val_acc)\n",
    "    avg_train_loss.append(train_loss.detach().item())\n",
    "    avg_test_loss.append(float(val_loss.item()))\n",
    "    print(f'Fold{fold}Append list is printing')\n",
    "    print(avg_train_acc,avg_test_acc,avg_test_loss,avg_train_loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ed7843-1292-408a-ae50-86f658ede254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalizationvalue(Train_dataloader):\n",
    "    import numpy as np\n",
    "    s1 = torch.tensor([0.0, 0.0, 0.0])\n",
    "    s2 = torch.tensor([0.0, 0.0, 0.0])\n",
    "    for image,_ in tqdm(Train_dataloader):\n",
    "    # print(image.size())\n",
    "        s1 += image.sum(axis=[0,2,3])\n",
    "        s2 += (image**2).sum(axis=[0,2,3])\n",
    "    n = LEN_TRAIN * image.size(2) * image.size(3)\n",
    "\n",
    "    mean = s1 / n\n",
    "    std = np.sqrt((s2/n) - ((s1/n)**2))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1578fa1-b580-424a-a889-05647d13affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path):\n",
    "    mean = [0.4757, 0.5001, 0.4264]\n",
    "    std = [0.2165, 0.1957, 0.2320]\n",
    "    transforms = transform.Compose([transform.Resize((256,256)), transform.ToTensor(), transform.Normalize(mean = mean, std = std)])\n",
    "    dataset = ImageFolder(root = path, transform = transforms)\n",
    "    return dataset\n",
    "    # StratifiedKFold(n_splits=split)\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81f2f6ed-db34-45a0-b8fa-6c92ea895056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    val_datasets = data_loader(val_data1)\n",
    "    train_data1 = int(0.95 * len(val_datasets))\n",
    "    testing_data1 = len(val_datasets) - train_data1\n",
    "    train_set1, val_set1 = torch.utils.data.random_split(val_datasets, [train_data1, testing_data1])\n",
    "    kfold = KFold(n_splits=3)\n",
    "    avg_train_acc = []\n",
    "    avg_test_acc = []\n",
    "    avg_train_loss = []\n",
    "    avg_test_loss = []\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(val_set1)):\n",
    "        print('*********************',fold)\n",
    "        train_sampler = SubsetRandomSampler(train_ids)\n",
    "        test_sampler = SubsetRandomSampler(test_ids)\n",
    "        Train_dataloader = DataLoader(val_set1, batch_size = 256, sampler = train_sampler)\n",
    "        Val_dataloader = DataLoader(val_set1, batch_size = 256, sampler = test_sampler)\n",
    "        # model = resnet()\n",
    "        training(3,Train_dataloader,Val_dataloader)\n",
    "        \n",
    "# model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af99709-789d-43f0-9ea3-d99a1a9321a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************* 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 26/26 [10:07<00:00, 23.35s/it, tr_acc=866, train_loss=tensor(3.2730, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Test_Loss: 3.2730398178100586,Validation_loss: 1586.259765625, Train_acc: 0.06571060019728356, Val_acc: 0.03732362312243969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:57<00:00, 23.00s/it, tr_acc=2046, train_loss=tensor(2.7966, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test_Loss: 2.796609878540039,Validation_loss: 3.363858699798584, Train_acc: 0.15524698383792396, Val_acc: 0.13154301319981793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.02s/it, tr_acc=3604, train_loss=tensor(2.1746, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Test_Loss: 2.1746387481689453,Validation_loss: 2.8764915466308594, Train_acc: 0.2734653615600577, Val_acc: 0.17410104688211198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:57<00:00, 22.99s/it, tr_acc=5824, train_loss=tensor(1.5219, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Test_Loss: 1.5218533277511597,Validation_loss: 2.835606813430786, Train_acc: 0.44191516807041503, Val_acc: 0.32976786527082386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:59<00:00, 23.05s/it, tr_acc=8088, train_loss=tensor(0.9834, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Test_Loss: 0.9834278225898743,Validation_loss: 2.1459286212921143, Train_acc: 0.6137036193944913, Val_acc: 0.4182976786527082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:01<00:00, 23.13s/it, tr_acc=9250, train_loss=tensor(0.8452, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Test_Loss: 0.8451547622680664,Validation_loss: 2.821169137954712, Train_acc: 0.7018741937931557, Val_acc: 0.44014565316340465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:00<00:00, 23.08s/it, tr_acc=10254, train_loss=tensor(0.6535, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Test_Loss: 0.6534925103187561,Validation_loss: 1.5682011842727661, Train_acc: 0.778055998178921, Val_acc: 0.566681838871188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:00<00:00, 23.10s/it, tr_acc=10923, train_loss=tensor(0.4644, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Test_Loss: 0.46440860629081726,Validation_loss: 1.1216700077056885, Train_acc: 0.8288185750056909, Val_acc: 0.742603550295858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.02s/it, tr_acc=11402, train_loss=tensor(0.4028, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Test_Loss: 0.4027918577194214,Validation_loss: 3.034024715423584, Train_acc: 0.8651642765004932, Val_acc: 0.553709604005462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:00<00:00, 23.10s/it, tr_acc=11686, train_loss=tensor(0.3122, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Test_Loss: 0.31219547986984253,Validation_loss: 0.4327267110347748, Train_acc: 0.8867137112072236, Val_acc: 0.8340919435593992\n",
      "Append list is printing\n",
      "[0.8867137112072236] [0.8340919435593992] [0.4327267110347748] [0.31219547986984253]\n",
      "********************* 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:01<00:00, 23.15s/it, tr_acc=616, train_loss=tensor(3.4966, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Test_Loss: 3.4965901374816895,Validation_loss: 557.3863525390625, Train_acc: 0.046737481031866465, Val_acc: 0.035511040291372635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:04<00:00, 23.26s/it, tr_acc=1415, train_loss=tensor(3.0742, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test_Loss: 3.074154853820801,Validation_loss: 5.384863376617432, Train_acc: 0.10735963581183612, Val_acc: 0.09219212383337128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:00<00:00, 23.09s/it, tr_acc=2612, train_loss=tensor(2.6293, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Test_Loss: 2.6292760372161865,Validation_loss: 3.1601181030273438, Train_acc: 0.19817905918057663, Val_acc: 0.165490553152743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:59<00:00, 23.04s/it, tr_acc=3922, train_loss=tensor(2.0766, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Test_Loss: 2.0765957832336426,Validation_loss: 27.013761520385742, Train_acc: 0.29757207890743553, Val_acc: 0.09924880491691328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:02<00:00, 23.19s/it, tr_acc=5542, train_loss=tensor(1.6553, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Test_Loss: 1.6552804708480835,Validation_loss: 2.154604196548462, Train_acc: 0.4204855842185129, Val_acc: 0.3578420213976781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.03s/it, tr_acc=7452, train_loss=tensor(1.1937, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Test_Loss: 1.1937140226364136,Validation_loss: 1.9042103290557861, Train_acc: 0.5654021244309559, Val_acc: 0.4493512406100615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.02s/it, tr_acc=9144, train_loss=tensor(0.8591, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Test_Loss: 0.8591396808624268,Validation_loss: 1.6746712923049927, Train_acc: 0.6937784522003035, Val_acc: 0.5297063510129751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.04s/it, tr_acc=9993, train_loss=tensor(0.6130, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Test_Loss: 0.6130279302597046,Validation_loss: 1.7340712547302246, Train_acc: 0.7581942336874051, Val_acc: 0.5763715001138174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.03s/it, tr_acc=10687, train_loss=tensor(0.4714, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Test_Loss: 0.4714486300945282,Validation_loss: 1.0427279472351074, Train_acc: 0.8108497723823975, Val_acc: 0.6924652856817665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:59<00:00, 23.05s/it, tr_acc=11440, train_loss=tensor(0.4129, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Test_Loss: 0.41286003589630127,Validation_loss: 0.8639750480651855, Train_acc: 0.8679817905918058, Val_acc: 0.7541543364443433\n",
      "Append list is printing\n",
      "[0.8867137112072236, 0.8679817905918058] [0.8340919435593992, 0.7541543364443433] [0.4327267110347748, 0.8639750480651855] [0.31219547986984253, 0.41286003589630127]\n",
      "********************* 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:59<00:00, 23.06s/it, tr_acc=379, train_loss=tensor(3.7740, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Test_Loss: 3.7740402221679688,Validation_loss: 2308.272216796875, Train_acc: 0.028755690440060697, Val_acc: 0.025495105850216252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:57<00:00, 23.00s/it, tr_acc=449, train_loss=tensor(3.6562, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test_Loss: 3.656200647354126,Validation_loss: 3.513033151626587, Train_acc: 0.03406676783004552, Val_acc: 0.0514454814477578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.01s/it, tr_acc=905, train_loss=tensor(3.1962, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Test_Loss: 3.196230888366699,Validation_loss: 4.355410575866699, Train_acc: 0.06866464339908952, Val_acc: 0.03824265877532438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.02s/it, tr_acc=2209, train_loss=tensor(2.4600, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Test_Loss: 2.4599769115448,Validation_loss: 3.7765369415283203, Train_acc: 0.16760242792109256, Val_acc: 0.11176872296835876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:59<00:00, 23.07s/it, tr_acc=4674, train_loss=tensor(1.8865, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Test_Loss: 1.8865242004394531,Validation_loss: 12.295341491699219, Train_acc: 0.35462822458270105, Val_acc: 0.1422717960391532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:57<00:00, 22.98s/it, tr_acc=6635, train_loss=tensor(1.5639, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Test_Loss: 1.5638586282730103,Validation_loss: 2.110433340072632, Train_acc: 0.5034142640364189, Val_acc: 0.36148417937628047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.03s/it, tr_acc=8225, train_loss=tensor(1.1059, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Test_Loss: 1.1059337854385376,Validation_loss: 2.0964908599853516, Train_acc: 0.624051593323217, Val_acc: 0.4429774641475074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:59<00:00, 23.05s/it, tr_acc=9338, train_loss=tensor(0.9671, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Test_Loss: 0.9670614004135132,Validation_loss: 1.5823228359222412, Train_acc: 0.7084977238239757, Val_acc: 0.5713635328932393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.03s/it, tr_acc=10273, train_loss=tensor(0.6331, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Test_Loss: 0.6330554485321045,Validation_loss: 0.8157057166099548, Train_acc: 0.7794385432473444, Val_acc: 0.6908718415661279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:56<00:00, 22.96s/it, tr_acc=10857, train_loss=tensor(0.4703, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Test_Loss: 0.470316082239151,Validation_loss: 1.144299864768982, Train_acc: 0.8237481031866465, Val_acc: 0.6667425449578875\n",
      "Append list is printing\n",
      "[0.8867137112072236, 0.8679817905918058, 0.8237481031866465] [0.8340919435593992, 0.7541543364443433, 0.6667425449578875] [0.4327267110347748, 0.8639750480651855, 1.144299864768982] [0.31219547986984253, 0.41286003589630127, 0.470316082239151]\n",
      "********************* 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.01s/it, tr_acc=559, train_loss=tensor(3.6541, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Test_Loss: 3.654127597808838,Validation_loss: 433.2011413574219, Train_acc: 0.04241274658573596, Val_acc: 0.027771454586842704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.02s/it, tr_acc=1336, train_loss=tensor(2.9485, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test_Loss: 2.9484992027282715,Validation_loss: 5.615000247955322, Train_acc: 0.10136570561456752, Val_acc: 0.06669701798315501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:59<00:00, 23.07s/it, tr_acc=3068, train_loss=tensor(2.3055, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Test_Loss: 2.305542469024658,Validation_loss: 3.0721168518066406, Train_acc: 0.23277693474962063, Val_acc: 0.19599362622353744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:57<00:00, 22.97s/it, tr_acc=5481, train_loss=tensor(1.6297, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Test_Loss: 1.629721999168396,Validation_loss: 2.282896041870117, Train_acc: 0.41585735963581183, Val_acc: 0.3184611882540405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:59<00:00, 23.06s/it, tr_acc=7347, train_loss=tensor(1.2131, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Test_Loss: 1.2130903005599976,Validation_loss: 2.0522243976593018, Train_acc: 0.5574355083459788, Val_acc: 0.41543364443432734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.00s/it, tr_acc=8604, train_loss=tensor(1.0324, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Test_Loss: 1.0324132442474365,Validation_loss: 2.931795358657837, Train_acc: 0.6528072837632777, Val_acc: 0.3808331436376053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:57<00:00, 22.99s/it, tr_acc=9483, train_loss=tensor(0.7445, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Test_Loss: 0.7444608211517334,Validation_loss: 2.7652604579925537, Train_acc: 0.7194992412746586, Val_acc: 0.4429774641475074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:58<00:00, 23.03s/it, tr_acc=10317, train_loss=tensor(0.6573, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Test_Loss: 0.6573319435119629,Validation_loss: 1.1200391054153442, Train_acc: 0.7827769347496206, Val_acc: 0.6628727521056226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:57<00:00, 22.98s/it, tr_acc=10765, train_loss=tensor(0.6459, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Test_Loss: 0.6458561420440674,Validation_loss: 2.2681260108947754, Train_acc: 0.8167678300455236, Val_acc: 0.5695424539039381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [09:57<00:00, 22.98s/it, tr_acc=11250, train_loss=tensor(0.3702, grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Test_Loss: 0.3702341318130493,Validation_loss: 0.8347463607788086, Train_acc: 0.8535660091047041, Val_acc: 0.7587070339175962\n",
      "Append list is printing\n",
      "[0.8867137112072236, 0.8679817905918058, 0.8237481031866465, 0.8535660091047041] [0.8340919435593992, 0.7541543364443433, 0.6667425449578875, 0.7587070339175962] [0.4327267110347748, 0.8639750480651855, 1.144299864768982, 0.8347463607788086] [0.31219547986984253, 0.41286003589630127, 0.470316082239151, 0.3702341318130493]\n"
     ]
    }
   ],
   "source": [
    "mean = [0.4757, 0.5001, 0.4264]\n",
    "std = [0.2165, 0.1957, 0.2320]\n",
    "\n",
    "transforms = transform.Compose([transform.Resize((256,256)), transform.ToTensor(), transform.Normalize(mean = mean, std = std)])\n",
    "val_datasets = ImageFolder(root = train_data1, transform=transforms)\n",
    "train_data1 = int(0.25 * len(val_datasets))\n",
    "testing_data1 = len(val_datasets) - train_data1\n",
    "train_set1, val_set1 = torch.utils.data.random_split(val_datasets, [train_data1, testing_data1])\n",
    "kfold = KFold(n_splits=4)\n",
    "avg_train_acc = []\n",
    "avg_test_acc = []\n",
    "avg_train_loss = []\n",
    "avg_test_loss = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_set1)):\n",
    "    print('*********************',fold)\n",
    "    train_sampler = SubsetRandomSampler(train_ids)\n",
    "    test_sampler = SubsetRandomSampler(test_ids)\n",
    "    Train_dataloader = DataLoader(train_set1, batch_size = 512, sampler = train_sampler)\n",
    "    Val_dataloader = DataLoader(train_set1, batch_size = 512, sampler = test_sampler)\n",
    "    # Creating model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "# model\n",
    "# Freeze all parameters in the pre-trained model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "# # Extracting features from last CNN layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features , 38)\n",
    "    # optimizer and loss function\n",
    "    # import torch.optim as optim\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    earlystopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(10):\n",
    "        start = time.time()\n",
    "    \n",
    "        tr_acc = 0\n",
    "        val_acc = 0\n",
    "\n",
    "    \n",
    "        # Train\n",
    "        model.train()\n",
    "        with tqdm(total=len(Train_dataloader)) as pbar:\n",
    "        #with tqdm(Train_dataloader, unit=\"batch\") as tepoch:\n",
    "            for xtrain, ytrain in Train_dataloader:\n",
    "            \n",
    "                train_prob = model(xtrain)\n",
    "        \n",
    "            \n",
    "                train_loss = loss_function(train_prob, ytrain)\n",
    "                train_loss.backward() # backpropagation\n",
    "\n",
    "            # loss.backward()\n",
    "                optimizer.step() # update the model parameters\n",
    "                optimizer.zero_grad()\n",
    "            # training ends\n",
    "            \n",
    "                train_pred = torch.max(train_prob, 1).indices\n",
    "                tr_acc += int(torch.sum(train_pred == ytrain))\n",
    "                \n",
    "                pbar.set_postfix(tr_acc = tr_acc, train_loss = train_loss)\n",
    "                pbar.update(n=1)\n",
    "            \n",
    "            ep_tr_acc = tr_acc / len(train_ids)\n",
    "        end = time.time()\n",
    "        duration = (end - start) / 60\n",
    "    \n",
    "    # print(f\"Epoch: {epoch}, Test_Loss: {train_loss}, Train_acc: {ep_tr_acc}\")\n",
    "    # Evaluate\n",
    "    \n",
    "        model.eval()\n",
    "#     # with torch.no_grad():\n",
    "        with torch.no_grad():\n",
    "            #with...\n",
    "            for xval, yval in Val_dataloader:\n",
    "                val_prob = model(xval)\n",
    "            \n",
    "                val_loss = loss_function(val_prob , yval)\n",
    "            \n",
    "                val_pred = torch.max(val_prob,1).indices\n",
    "                val_acc += int(torch.sum(val_pred == yval))\n",
    "            \n",
    "            ep_val_acc = val_acc / len(test_ids)\n",
    "        # avg_val_loss = val_loss / len(Val_dataloader)\n",
    "        earlystopping._stopping(val_loss)\n",
    "        if earlystopping.stop:\n",
    "            print('Early Stopping Excuated')\n",
    "            break\n",
    "        end = time.time()\n",
    "        duration = (end - start) / 60\n",
    "    \n",
    "        print(f\"Epoch: {epoch}, Test_Loss: {train_loss},Validation_loss: {val_loss}, Train_acc: {ep_tr_acc}, Val_acc: {ep_val_acc}\")\n",
    "    avg_train_acc.append(ep_tr_acc)\n",
    "    avg_test_acc.append(ep_val_acc)\n",
    "    avg_train_loss.append(train_loss.detach().item())\n",
    "    avg_test_loss.append(float(val_loss.item()))\n",
    "    print('Append list is printing')\n",
    "    print(avg_train_acc,avg_test_acc,avg_test_loss,avg_train_loss)\n",
    "\n",
    "\n",
    "    \n",
    "# print(len(train_ids))\n",
    "# print(len(test_ids))\n",
    "    \n",
    "# Val_dataloader = DataLoader(dataset = val_set1,  batch_size= 256, shuffle = False )\n",
    "# validation_set1 = Val_dataloader.dataset\n",
    "\n",
    "# len(validation_set1),len(val_set1),len(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed688a-8660-4fd3-bc6e-0ea83c4434c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = [0.4757, 0.5001, 0.4264]\n",
    "# std = [0.2165, 0.1957, 0.2320]\n",
    "\n",
    "# transforms = transform.Compose([transform.Resize((256,256)), transform.ToTensor(), transform.Normalize(mean = mean, std = std)])\n",
    "# val_datasets = ImageFolder(root = train_data1, transform=transforms)\n",
    "# train_data1 = int(0.80 * len(val_datasets))\n",
    "# testing_data1 = len(val_datasets) - train_data1\n",
    "# train_set1, val_set1 = torch.utils.data.random_split(val_datasets, [train_data1, testing_data1])\n",
    "# kfold = KFold(n_splits=10)\n",
    "# sec_avg_train_acc = []\n",
    "# sec_avg_test_acc = []\n",
    "# sec_avg_train_loss = []\n",
    "# sec_avg_test_loss = []\n",
    "# for fold, (train_ids, test_ids) in enumerate(kfold.split(train_set1)):\n",
    "#     print('*********************',fold)\n",
    "#     train_sampler = SubsetRandomSampler(train_ids)\n",
    "#     test_sampler = SubsetRandomSampler(test_ids)\n",
    "#     Train_dataloader = DataLoader(train_set1, batch_size = 256, sampler = train_sampler)\n",
    "#     Val_dataloader = DataLoader(train_set1, batch_size = 256, sampler = test_sampler)\n",
    "#     # from torchvision import models\n",
    "#     # import torch.nn as nn\n",
    "#     model = models.resnet18(pretrained=True)\n",
    "# # model\n",
    "# # Freeze all parameters in the pre-trained model\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = True\n",
    "# # # Extracting features from last CNN layer\n",
    "#     num_features = model.fc.in_features\n",
    "#     model.fc = nn.Linear(num_features , 38)\n",
    "#     # optimizer and loss function\n",
    "#     # import torch.optim as optim\n",
    "#     optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "#     loss_function = nn.CrossEntropyLoss()\n",
    "#     earlystopping = EarlyStopping(patience=5)\n",
    "#     for epoch in range(30):\n",
    "#         start = time.time()\n",
    "    \n",
    "#         tr_acc = 0\n",
    "#         val_acc = 0\n",
    "\n",
    "    \n",
    "#     # Train\n",
    "#         model.train()\n",
    "    \n",
    "#         with tqdm(Train_dataloader, unit=\"batch\") as tepoch:\n",
    "#             for xtrain, ytrain in tepoch:\n",
    "            \n",
    "#                 train_prob = model(xtrain)\n",
    "        \n",
    "            \n",
    "#                 train_loss = loss_function(train_prob, ytrain)\n",
    "#                 train_loss.backward() # backpropagation\n",
    "\n",
    "#             # loss.backward()\n",
    "#                 optimizer.step() # update the model parameters\n",
    "#                 optimizer.zero_grad()\n",
    "#             # training ends\n",
    "            \n",
    "#                 train_pred = torch.max(train_prob, 1).indices\n",
    "#                 tr_acc += int(torch.sum(train_pred == ytrain))\n",
    "            \n",
    "#             ep_tr_acc = tr_acc / len(train_ids)\n",
    "#         end = time.time()\n",
    "#         duration = (end - start) / 60\n",
    "    \n",
    "#     # print(f\"Epoch: {epoch}, Test_Loss: {train_loss}, Train_acc: {ep_tr_acc}\")\n",
    "#     # Evaluate\n",
    "    \n",
    "#         model.eval()\n",
    "# #     # with torch.no_grad():\n",
    "#         with torch.no_grad():\n",
    "#             for xval, yval in Val_dataloader:\n",
    "#                 val_prob = model(xval)\n",
    "            \n",
    "#                 val_loss = loss_function(val_prob , yval)\n",
    "            \n",
    "#                 val_pred = torch.max(val_prob,1).indices\n",
    "#                 val_acc += int(torch.sum(val_pred == yval))\n",
    "            \n",
    "#             ep_val_acc = val_acc / len(test_ids)\n",
    "#         # avg_val_loss = val_loss / len(Val_dataloader)\n",
    "#         earlystopping._stopping(val_loss)\n",
    "#         if earlystopping.stop:\n",
    "#             print('Early Stopping Excuated')\n",
    "#             break\n",
    "#         end = time.time()\n",
    "#         duration = (end - start) / 60\n",
    "    \n",
    "#         print(f\"Epoch: {epoch}, Test_Loss: {train_loss},Validation_loss: {val_loss}, Train_acc: {ep_tr_acc}, Val_acc: {ep_val_acc}\")\n",
    "#     sec_avg_train_acc.append(ep_tr_acc)\n",
    "#     sec_avg_test_acc.append(ep_val_acc)\n",
    "#     sec_avg_train_loss.append(train_loss.detach().item())\n",
    "#     sec_avg_test_loss.append(float(val_loss.item()))\n",
    "#     print('Append list is printing')\n",
    "#     print(sec_avg_train_acc,sec_avg_test_acc,sec_avg_train_loss,sec_avg_test_loss)\n",
    "\n",
    "\n",
    "    \n",
    "# # print(len(train_ids))\n",
    "# # print(len(test_ids))\n",
    "    \n",
    "# # Val_dataloader = DataLoader(dataset = val_set1,  batch_size= 256, shuffle = False )\n",
    "# # validation_set1 = Val_dataloader.dataset\n",
    "\n",
    "# # len(validation_set1),len(val_set1),len(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72709cd5-1396-4d55-8e17-eb147c362262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(avg_train_acc), np.mean(avg_test_acc), np.mean(avg_test_loss), np.mean(avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ab2c1-55be-4e92-be2a-052290b04edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset = torch.tensor([ 9, 8,7,6,5,4,3,2,1])\n",
    "# # Define the indices of the subset you want to sample from the dataset\n",
    "# # train_ids = [0, 1, 2]  # Example subset indices\n",
    "# kfold = KFold(n_splits=3)\n",
    "# for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "#     # Print\n",
    "#     print(f'FOLD {fold}')\n",
    "#     # print(\n",
    "# # Create a SubsetRandomSampler using the train_ids\n",
    "#     sampler = SubsetRandomSampler(train_ids)\n",
    "#     test_sampler       =  SubsetRandomSampler(test_ids)\n",
    "# # Create a DataLoader using the sampler\n",
    "#     dataloader = DataLoader(dataset, batch_size=32,sampler=sampler)\n",
    "#     for batch in dataloader:\n",
    "#         print(batch)\n",
    "#     dataloader = DataLoader(dataset, batch_size=32, sampler=test_sampler)\n",
    "#     for batch in dataloader:\n",
    "#         print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dcf962-55db-4122-8c34-18af881af635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
